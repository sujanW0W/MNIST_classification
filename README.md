# 🧠 MNIST Digit Classifier – Softmax Regression from Scratch (with PyTorch!)

Hey there! 👋  
Welcome to my mini deep learning adventure! This project is all about classifying handwritten digits from the MNIST dataset using **softmax regression**, built from scratch using **PyTorch** – no high-level `nn.Module`, just raw tensors and logic.

I’ve kept things minimal on purpose — just an **input layer**, an **output layer**, and a lot of heart 💪.

---

## ✨ What’s Inside?

-   🔢 Manual implementation of a simple neural net for multiclass classification
-   🔥 Softmax + Cross-Entropy Loss = Clean predictions
-   📦 Batch training with size `64`
-   🧪 Trained over **1000 epochs** on the MNIST dataset
-   🧠 Achieved **~90–92% accuracy** on the test set
-   📓 Jupyter Notebook with code, plots, and explanations

---

## 🗂️ Dataset

You’ll need to grab the MNIST dataset in binary format from Kaggle:  
🔗 [Kaggle – MNIST](https://www.kaggle.com/datasets/hojjatk/mnist-dataset)
