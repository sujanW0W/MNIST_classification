# ğŸ§  MNIST Digit Classifier â€“ Softmax Regression from Scratch (with PyTorch!)

Hey there! ğŸ‘‹  
Welcome to my mini deep learning adventure! This project is all about classifying handwritten digits from the MNIST dataset using **softmax regression**, built from scratch using **PyTorch** â€“ no high-level `nn.Module`, just raw tensors and logic.

Iâ€™ve kept things minimal on purpose â€” just an **input layer**, an **output layer**, and a lot of heart ğŸ’ª.

---

## âœ¨ Whatâ€™s Inside?

-   ğŸ”¢ Manual implementation of a simple neural net for multiclass classification
-   ğŸ”¥ Softmax + Cross-Entropy Loss = Clean predictions
-   ğŸ“¦ Batch training with size `64`
-   ğŸ§ª Trained over **1000 epochs** on the MNIST dataset
-   ğŸ§  Achieved **~90â€“92% accuracy** on the test set
-   ğŸ““ Jupyter Notebook with code, plots, and explanations

---

## ğŸ—‚ï¸ Dataset

Youâ€™ll need to grab the MNIST dataset in binary format from Kaggle:  
ğŸ”— [Kaggle â€“ MNIST](https://www.kaggle.com/datasets/hojjatk/mnist-dataset)
